{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "568de1c9-1756-47db-9e0d-37f697a30814",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%init_spark --stderr\n",
    "launcher.conf.spark.app.name = \"DIC Exercise 2\"\n",
    "launcher.conf.spark.default.parallelism = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ca21176-051a-419a-8e0f-30aba87953c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://captain01.os.hpc.tuwien.ac.at:9999/proxy/application_1715326141961_0076\n",
       "SparkContext available as 'sc' (version = 3.2.3, master = yarn, app id = application_1715326141961_0076)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.app.name: DIC Exercise 2\n",
      "spark.master: yarn\n",
      "spark.submit.deployMode: client\n",
      "sc.defaultParallelism: 12\n"
     ]
    }
   ],
   "source": [
    "println(\"spark.app.name: \" + spark.conf.get(\"spark.app.name\"))\n",
    "println(\"spark.master: \" + spark.conf.get(\"spark.master\"))\n",
    "println(\"spark.submit.deployMode: \" + spark.conf.get(\"spark.submit.deployMode\"))\n",
    "println(\"sc.defaultParallelism: \" + sc.defaultParallelism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf38802b-d476-49eb-ab2b-c135714e3d2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "\n",
      "\n",
      "-rw-r--r--   3 blee supergroup      55.3 M 2023-03-13 11:18 /user/dic24_shared/amazon-reviews/full/reviews_devset.json\n",
      "\n",
      "\n",
      "-rw-r--r--   3 blee supergroup      54.3 G 2023-03-13 11:18 /user/dic24_shared/amazon-reviews/full/reviewscombined.json\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls -h /user/dic24_shared/amazon-reviews/full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "382d2bb0-52d3-4fc9-9ea2-5386e0ccf795",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                         (0 + 2) / 12]\n",
      "[Stage 0:==============>                                           (3 + 2) / 12]\n",
      "[Stage 0:=================================>                        (7 + 2) / 12]\n",
      "                                                                                \n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "text/plain": [
       "reviews: org.apache.spark.sql.DataFrame = [asin: string, category: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//val textFile = sc.textFile(\"hdfs:///user/dic24_shared/amazon-reviews/full/reviews_devset.json\")\n",
    "\n",
    "val reviews = spark.read.json(\"hdfs:///user/dic24_shared/amazon-reviews/full/reviews_devset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe1121be-8366-41f0-83eb-21683e281eb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------+\n",
      "|      asin|            category| helpful|overall|          reviewText| reviewTime|    reviewerID|        reviewerName|             summary|unixReviewTime|\n",
      "+----------+--------------------+--------+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------+\n",
      "|0981850006|Patio_Lawn_and_Garde|  [6, 7]|    5.0|This was a gift f...| 12 3, 2009|A2VNYWOPJ13AFP|Amazon Customer \"...|              Delish|    1259798400|\n",
      "|B00002N66D|Patio_Lawn_and_Garde|  [1, 1]|    5.0|This is a very ni...| 12 3, 2012|A2E5XXXC07AGA7|               James|       Nice spreader|    1354492800|\n",
      "|B00002N67U|Patio_Lawn_and_Garde|  [0, 1]|    1.0|The metal base wi...|08 13, 2008|A16PX63WZIEQ13|             Finaldx| Terrible spike base|    1218585600|\n",
      "|B00002N6AN|Patio_Lawn_and_Garde|  [0, 0]|    4.0|For the most part...| 10 1, 2009|A2OSWM3522VARA|Wayne Allen \"Moto...|   gets the job done|    1254355200|\n",
      "|B00002N8K3|Patio_Lawn_and_Garde|  [4, 5]|    1.0|This hose is supp...|07 13, 2013|A2SX9YPPGEUADI|HappyCamper \"Happ...|           The worst|    1373673600|\n",
      "|B00002NBQ8|Patio_Lawn_and_Garde|  [2, 2]|    5.0|This tool works v...| 04 1, 2014|A2PENG0PDZUEGN|John Grossbohlin ...|          Great tool|    1396310400|\n",
      "|B00004DTNG|Patio_Lawn_and_Garde|  [1, 2]|    4.0|This product is a...|10 31, 2008|A2NBUMLJ0QBTND|            L. Allen|Black & Decker El...|    1225411200|\n",
      "|B00004OCJZ|Patio_Lawn_and_Garde|[10, 12]|    1.0|I was excited to ...|12 15, 2011| AC9EDJLYU6DEH|               NandM|Seems Like Good I...|    1323907200|\n",
      "|B00004R9TJ|Patio_Lawn_and_Garde|  [0, 1]|    5.0|I purchased the L...|11 28, 2009|A2OKI2AQ4QQV8R|          Ian Koenig|  Speeds up mulching|    1259366400|\n",
      "|B00004R9UK|Patio_Lawn_and_Garde|  [0, 0]|    4.0|Never used a manu...| 09 8, 2013|A1DYWRLW6ZB4FW|          guitarhero|  Works as described|    1378598400|\n",
      "|B00004R9VV|Patio_Lawn_and_Garde|  [1, 1]|    5.0|Good price. Good ...| 11 6, 2013|A3D8QKK1Z19XMM|         Craig S Fry|           Satisfied|    1383696000|\n",
      "|B00004R9VV|Patio_Lawn_and_Garde|[13, 13]|    5.0|I have owned the ...|04 12, 2008| A2OAGDTF00OVD|           M. Roldan|Flowtron 15 watt ...|    1207958400|\n",
      "|B00004R9XC|Patio_Lawn_and_Garde|  [2, 2]|    5.0|I had \"won\" a sim...| 09 9, 2009|A1V24A11LXC8FM|          L. Edwards|   Multipurpose ties|    1252454400|\n",
      "|B00004RA0O|Patio_Lawn_and_Garde|  [0, 0]|    4.0|The birds ate all...|07 18, 2013|A1ZFSH5UIXYC5Y|         Music lover|This netting does...|    1374105600|\n",
      "|B00004RA3E|Patio_Lawn_and_Garde|[12, 13]|    5.0|Bought last summe...|03 30, 2005|A1D8C0T4T353M9|Lives up North \"E...|Perfect for right...|    1112140800|\n",
      "|B00004RA4G|Patio_Lawn_and_Garde|  [0, 0]|    4.0|I knew I had a mo...| 06 3, 2014|A33ML4YVHKCTQH|            J. Brody|Be patient--it do...|    1401753600|\n",
      "|B00004RA88|Patio_Lawn_and_Garde|  [0, 0]|    4.0|I was a little wo...|04 10, 2013|A34Y0G9RRXQXZJ|             Greg S.|      Chainsaw Chain|    1365552000|\n",
      "|B00004RA91|Patio_Lawn_and_Garde| [8, 10]|    5.0|I have used this ...| 07 7, 2000|A2FWAQS2ZYELDY|             Lisa H.|  Hummingbird Nectar|     962928000|\n",
      "|B00004RAL1|Patio_Lawn_and_Garde|[10, 10]|    5.0|I actually do not...|02 18, 2001|A3QBS6EPFZZH6V|        Nancy Phipps|Quality is the an...|     982454400|\n",
      "|B00004RALL|Patio_Lawn_and_Garde|  [0, 0]|    5.0|Just what I  expe...|05 16, 2013|A1N7X8D30UA8X7|                 mel|            Mini BBQ|    1368662400|\n",
      "+----------+--------------------+--------+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcca6ffb-1222-4f98-9b0e-f320d0f021bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dic24/e12347510/Exercise_2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2731ffbd-1d9a-429c-aaf6-0ee45d0b19c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stopwordsSource: scala.io.BufferedSource = <iterator>\n",
       "stopwords: scala.collection.immutable.Set[String] = Set(serious, latterly, absorbs, looks, particularly, used, e, printer, down, regarding, entirely, regardless, moreover, please, read, ourselves, able, behind, for, despite, s, maybe, viz, further, corresponding, x, any, wherein, across, name, allows, this, instead, in, taste, ought, myself, have, your, off, once, are, is, mon, his, oh, why, rd, knows, bulbs, too, among, course, greetings, somewhat, bibs, everyone, seen, likely, said, try, already, soon, nobody, got, given, song, using, less, am, consider, hence, than, n, accordingly, four, anyhow, want, three, forth, whereby, himself, specify, yes, throughout, inasmuch, but, whether, sure, below, aren, co, best, plus, becomes, what,...\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stopwordsSource = scala.io.Source.fromFile(\"/home/dic24/e12347510/Exercise_2/data/stopwords.txt\")\n",
    "\n",
    "// Convert the Source to a Set for efficient lookup\n",
    "val stopwords = stopwordsSource.getLines.toSet\n",
    "\n",
    "// Close the Source object to release resources\n",
    "stopwordsSource.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c403122-8367-41df-8551-9731a0afe282",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:========================>                                 (5 + 3) / 12]\n",
      "[Stage 2:===========================================>              (9 + 2) / 12]\n",
      "                                                                                \n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "text/plain": [
       "reviewTextsDF: org.apache.spark.sql.DataFrame = [reviewText: string]\n",
       "reviewTextsArray: Array[String] = Array(This was a gift for my other husband.  He's making us things from it all the time and we love the food.  Directions are simple, easy to read and interpret, and fun to make.  We all love different kinds of cuisine and Raichlen provides recipes from everywhere along the barbecue trail as he calls it. Get it and just open a page.  Have at it.  You'll love the food and it has provided us with an insight into the culture that produced it. It's all about broadening horizons.  Yum!!, This is a very nice spreader.  It feels very solid and the pneumatic tires give it great maneuverability and handling over bumps.  The control arm is solid metal, not a cable, which gives you precise contro...\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Extract the reviewText attribute\n",
    "val reviewTextsDF = reviews.select(\"reviewText\")\n",
    "\n",
    "// Convert DataFrame to Array[String]\n",
    "val reviewTextsArray = reviewTextsDF.map(row => row.getString(0)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f9eaea4-fa43-4425-be24-15e99012f94a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizeAndRemoveDuplicates: (reviewText: String)Seq[String]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenizeAndRemoveDuplicates(reviewText: String): Seq[String] = {\n",
    "    // Split the review text into tokens using regular expression\n",
    "    val tokens = reviewText.toLowerCase.split(\"[^a-zA-Z<>^|]+\")\n",
    "\n",
    "    // Remove duplicates\n",
    "    tokens.distinct.filter(token => token.length > 1 && !stopwords.contains(token))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b1cc36e-eaf6-472c-9359-ba6582c7edc4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizedReviewsRDD: Array[Seq[String]] = Array(WrappedArray(gift, husband, making, things, time, love, food, directions, simple, easy, interpret, make, kinds, cuisine, raichlen, recipes, barbecue, trail, calls, open, page, provided, insight, culture, produced, broadening, horizons, yum), WrappedArray(nice, spreader, feels, solid, pneumatic, tires, give, great, maneuverability, handling, bumps, control, arm, metal, cable, precise, long, time, settings, experimentation, products, true, good, distribution, flings, material, farther, side, left, crappy, edgeguard), WrappedArray(metal, base, hose, attachments, poorly, designed, made, previous, reviewer, pointed, leaks, badly, pops, fix, tighten, junction, plastic, wears, bit, falls, sprinkler, useless, avoid, gilmour, sprinklers, spike, hea...\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Apply the function to each review text using map\n",
    "val tokenizedReviewsRDD = reviewTextsArray.map(tokenizeAndRemoveDuplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "458a20dd-27e1-4712-babb-a9801144727a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "totalCount: Int = 2123321\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val totalCount = tokenizedReviewsRDD.map(_.length).sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a73b4743-ca69-478d-9623-92c2ded9ccf0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "categoriesDF: org.apache.spark.sql.DataFrame = [category: string]\n",
       "categoriesArray: Array[String] = Array(Patio_Lawn_and_Garde, Patio_Lawn_and_Garde, Patio_Lawn_and_Garde, Patio_Lawn_and_Garde, Patio_Lawn_and_Garde, Patio_Lawn_and_Garde, Patio_Lawn_and_Garde, Patio_Lawn_and_Garde, Patio_Lawn_and_Garde, Patio_Lawn_and_Garde, Patio_Lawn_and_Garde, Patio_Lawn_and_Garde, Patio_Lawn_and_Garde, Patio_Lawn_and_Garde, Patio_Lawn_and_Garde, Patio_Lawn_and_Garde, Patio_Lawn_and_Garde, Patio_Lawn_and_Garde, Patio_Lawn_and_Garde, Patio_Lawn_and_Garde, Patio_Lawn_and_Garde, Patio_Lawn_and_Garde, Patio_Lawn_and_Garde, Patio_Lawn_and_Garde, Patio_Lawn_and_Garde, Patio_Lawn_and_Garde, Patio_Lawn_and_Garde, Patio_Lawn_and_Garde, Patio_Lawn_and_Garde, Patio_Lawn_and_Garde, Patio_Lawn_and_Garde, Patio_Lawn...\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Extract the category attribute\n",
    "val categoriesDF = reviews.select(\"category\")\n",
    "\n",
    "// Convert DataFrame to Array[String]\n",
    "val categoriesArray = categoriesDF.map(row => row.getString(0)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21a63b76-b33a-45a7-b952-30840b4cb224",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "categoryReviewPairsRDD: Array[(String, Seq[String])] = Array((Patio_Lawn_and_Garde,WrappedArray(gift, husband, making, things, time, love, food, directions, simple, easy, interpret, make, kinds, cuisine, raichlen, recipes, barbecue, trail, calls, open, page, provided, insight, culture, produced, broadening, horizons, yum)), (Patio_Lawn_and_Garde,WrappedArray(nice, spreader, feels, solid, pneumatic, tires, give, great, maneuverability, handling, bumps, control, arm, metal, cable, precise, long, time, settings, experimentation, products, true, good, distribution, flings, material, farther, side, left, crappy, edgeguard)), (Patio_Lawn_and_Garde,WrappedArray(metal, base, hose, attachments, poorly, designed, made, previous, reviewer, pointed, leaks, badly, pops, fix, tighten, junction, plast...\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val categoryReviewPairsRDD = categoriesArray.zip(tokenizedReviewsRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b441aa7-c46c-41fe-8b2c-2f7124a1b5d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "categoryTermPairsRDD: org.apache.spark.rdd.RDD[((String, String), Int)] = ParallelCollectionRDD[16] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val categoryTermPairsRDD = sc.parallelize(categoryReviewPairsRDD.flatMap {\n",
    "  case (category, reviewTextArray) =>\n",
    "    reviewTextArray.map(term => ((category, term),1))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb9f6dbd-01bc-42dd-b43b-4f68b76d5495",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;31m24/05/11 17:33:11 WARN TaskSetManager: Stage 4 contains a task of very large size (5489 KiB). The maximum recommended task size is 1000 KiB.\n",
      "\n",
      "[Stage 4:>                                                         (0 + 1) / 12]\n",
      "[Stage 4:>                                                         (0 + 2) / 12]\n",
      "[Stage 4:====>                                                     (1 + 2) / 12]\n",
      "[Stage 4:=========>                                                (2 + 2) / 12]\n",
      "[Stage 4:==============>                                           (3 + 2) / 12]\n",
      "[Stage 4:===================>                                      (4 + 2) / 12]\n",
      "[Stage 4:========================>                                 (5 + 2) / 12]\n",
      "[Stage 4:=============================>                            (6 + 2) / 12]\n",
      "[Stage 4:=================================>                        (7 + 2) / 12]\n",
      "[Stage 4:======================================>                   (8 + 2) / 12]\n",
      "[Stage 4:===========================================>              (9 + 2) / 12]\n",
      "[Stage 4:====================================================>    (11 + 1) / 12]\n",
      "[Stage 5:>                                                         (0 + 2) / 12]\n",
      "[Stage 5:>                                                         (0 + 4) / 12]\n",
      "[Stage 5:=========>                                                (2 + 2) / 12]\n",
      "[Stage 5:===================>                                      (4 + 2) / 12]\n",
      "[Stage 5:========================>                                 (5 + 3) / 12]\n",
      "[Stage 5:=============================>                            (6 + 2) / 12]\n",
      "[Stage 5:======================================>                   (8 + 2) / 12]\n",
      "[Stage 5:===============================================>         (10 + 2) / 12]\n",
      "                                                                                \n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "text/plain": [
       "groupedRDD: org.apache.spark.rdd.RDD[((String, String), Iterable[Int])] = ShuffledRDD[17] at groupByKey at <console>:25\n",
       "A: Array[((String, String), Int)] = Array(((Automotive,set),50), ((Electronic,hopes),18), ((Book,salaciously),1), ((Movies_and_TV,sophomoric),1), ((Book,birds),49), ((Electronic,hvr),2), ((Book,marketeering),1), ((Movies_and_TV,sabbath),4), ((Book,mccullough),4), ((Office_Product,funky),2), ((Book,comprehension),22), ((Book,operative),6), ((Book,vengeful),8), ((Movies_and_TV,straight),45), ((Movies_and_TV,cougar),1), ((Book,chap),3), ((Book,gwynne),1), ((Automotive,arerunning),1), ((Kindle_Store,posters),1), ((Movies_and_TV,karajan),1), ((Book,regulation),10), ((Tools_and_Home_Improvement,buyer),4), ((Movies_and_TV,bells),1), ((Book,ot),13), ((Book,native),93), ((Movie...\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Group by key (category-term)\n",
    "val groupedRDD = categoryTermPairsRDD.groupByKey()\n",
    "\n",
    "// Reduce by key to count the occurrences of each term within each category\n",
    "val A = groupedRDD.mapValues(iterable => iterable.sum).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93f98bd6-fb06-42de-82a5-b7851b8257b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:=========>                                                (2 + 2) / 12]\n",
      "[Stage 6:========================>                                 (5 + 2) / 12]\n",
      "[Stage 6:======================================>                   (8 + 2) / 12]\n",
      "                                                                                \n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "text/plain": [
       "groupedByTerms: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[20] at map at <console>:26\n",
       "termGroupedRDD: org.apache.spark.rdd.RDD[(String, Iterable[Int])] = ShuffledRDD[21] at groupByKey at <console>:29\n",
       "termTotal: Array[(String, Int)] = Array((vga,32), (mattered,12), (intimately,18), (apostrophes,6), (reunion,46), (bone,92), (valorie,2), (sandberg,1), (qraitians,1), (showy,4), (bombast,3), (folan,1), (hermeneutical,3), (xxoo,1), (dole,1), (adverising,1), (gowns,5), (ducklings,1), (tamanu,1), (ensnare,1), (modifies,2), (iturbide,1), (gayton,1), (omnipresence,1), (crying,98), (boucle,1), (contemptible,3), (semites,1), (surgeplus,1), (husky,17), (artur,3), (merchandising,1), (beauracy,1), (dnr,3), (ranfly,1), (beleive,13), (ior,1), (grant,73), (unitentional,1), (horvatic,1), (...\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Convert the array A to an RDD of key-value pairs\n",
    "val groupedByTerms = sc.parallelize(A).map { case ((cat, term), value) => (term, value) }\n",
    "\n",
    "// Group by key (term)\n",
    "val termGroupedRDD = groupedByTerms.groupByKey()\n",
    "\n",
    "// Reduce by key to sum up the values for each term\n",
    "val termTotal = termGroupedRDD.mapValues(_.sum).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42116233-dfd9-47a2-9499-e8e3f4f927c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A_RDD: org.apache.spark.rdd.RDD[((String, String), Int)] = ParallelCollectionRDD[23] at parallelize at <console>:27\n",
       "termTotal_RDD: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[24] at parallelize at <console>:28\n",
       "A_Keyed_RDD: org.apache.spark.rdd.RDD[(String, (String, Int))] = MapPartitionsRDD[25] at flatMap at <console>:31\n",
       "termTotal_Keyed_RDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[26] at map at <console>:32\n",
       "B_RDD: org.apache.spark.rdd.RDD[((String, String), (Int, Int))] = MapPartitionsRDD[27] at mapPartitions at <console>:35\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Convert A and termTotal to RDDs\n",
    "val A_RDD = sc.parallelize(A)\n",
    "val termTotal_RDD = sc.parallelize(termTotal)\n",
    "\n",
    "// Convert A_RDD and termTotal_RDD to key-value pairs\n",
    "val A_Keyed_RDD = A_RDD.flatMap { case ((category, term), value) => Some((term, (category, value))) }\n",
    "val termTotal_Keyed_RDD = termTotal_RDD.map { case (term, value) => (term, value) }\n",
    "\n",
    "// Calculate the values for each category-term pair in B\n",
    "val B_RDD = A_Keyed_RDD.mapPartitions(iter => {\n",
    "  val termTotalMap = iter.map { case (term, (category, valueA)) => (term, valueA) }.toMap\n",
    "  iter.map { case (term, (category, valueA)) =>\n",
    "    val valueTotal = termTotalMap.getOrElse(term, 0)\n",
    "    val valueB = valueTotal - valueA\n",
    "    ((category, term), (valueA, valueB))\n",
    "  }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "478137ce-fadb-4021-ba83-9e838d3cfc52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A_and_B: Array[((String, String), (Int, Int))] = Array()\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val A_and_B = B_RDD.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
